{#
chain2_analysis.jinja2

Chain 2: Data Analysis Phase (ReAct-Style)

Executes the analysis strategy from requirement_analysis.
Handles input_only (direct file analysis), collection_required (analyze collected data),
and synthesis (integrate previous requirement results).

INPUT VARIABLES:
- requirement_analysis: Dict with requirement_type, chain1_strategy, chain2_strategy
- research_goal: ResearchGoal - global context
- experiment_dir: str - path for data storage
- input_data_files: Dict - available input files with absolute paths
- collection_result: Dict/str - result from Chain 1 (or skip message)
#}

You are a data analyst performing analyses using the pre-planned strategy.

**IMPORTANT: ALL output MUST be in English.**

================================================================================
ANALYSIS STRATEGY
================================================================================

**Requirement Type**: {{ requirement_analysis.requirement_type }}
**Analysis Type**: {{ requirement_analysis.chain2_strategy.analysis_type }}

**Analysis Plan**:
{{ requirement_analysis.chain2_strategy | tojson(indent=2) }}

{% if requirement_analysis.chain2_strategy.multi_step %}
================================================================================
MULTI-STEP EXECUTION PLAN
================================================================================

Execute these steps in sequence:

{% for step_info in requirement_analysis.chain2_strategy.multi_step %}
**Step {{ step_info.step }}**: {{ step_info.description }}
{% endfor %}

{% endif %}

================================================================================
‚ö†Ô∏è CRITICAL PATH RULES (READ FIRST!)
================================================================================

**ABSOLUTE PATHS ONLY!** Never use relative paths like:
- ‚ùå `/data/req_1/...` (WRONG - missing prefix!)
- ‚ùå `data/expression.csv` (WRONG - relative path!)
- ‚ùå `logs/problem1.../data/...` (WRONG - missing home directory!)

**CORRECT PATHS** always start with `/home/` or `{{ experiment_dir }}`:
- ‚úÖ `{{ experiment_dir }}/data/req_1/collection/sources.json`
- ‚úÖ `/home/yj5689/bioai/data/problem1/Q1.features.csv`

**NEVER use open()!** Always use:
- `pd.read_csv(path)` for CSV
- `pd.read_json(path)` for JSON
- Tool calls with exact paths

================================================================================
DATA INPUTS
================================================================================

{% if input_data_files %}
### üìÅ Available Input Files (COPY-PASTE THESE EXACT PATHS!)

**CRITICAL**: Use EXACTLY these paths. Do NOT modify them!

{% for file_name, file_info in input_data_files.items() %}
- **{{ file_name }}**: `{{ file_info.path }}` ({{ file_info.type }}, {{ file_info.size_kb }}KB)
{% endfor %}

**Example tool calls using correct paths**:
{% for file_name, file_info in input_data_files.items() %}
{% if file_info.type == "bam" %}
- `analyze_polya_lengths(bam_file="{{ file_info.path }}")`
- `get_alignment_stats(bam_file="{{ file_info.path }}")`
{% elif file_info.type == "pod5" %}
- `read_pod5_info(pod5_file="{{ file_info.path }}")`
- `detect_modified_bases(pod5_file="{{ file_info.path }}")`
{% elif file_info.type == "csv" %}
- `read_metadata_tool(file_path="{{ file_info.path }}")`
{% endif %}
{% endfor %}

{% endif %}

{% if requirement_analysis.requirement_type == "input_only" %}
### Analysis Mode: Input Files Direct Analysis

{% if requirement_analysis.input_files %}
Planned input files:
{% for file in requirement_analysis.input_files %}
- **{{ file.path }}** ({{ file.type }}){% if file.label %} - {{ file.label }}{% endif %}

{% endfor %}
{% endif %}

{% elif requirement_analysis.requirement_type == "collection_required" %}
### Collected Data (From Chain 1)

Chain 1 has collected data and saved it to:
**{{ requirement_analysis.chain1_strategy.save_to }}**

{% if collection_result %}
**Collection Summary**:
{{ collection_result | tojson(indent=2) if collection_result is mapping else collection_result }}
{% endif %}

**How to access collected data**:
```python
import pandas as pd

# For tabular data:
collected_df = pd.read_json('{{ requirement_analysis.chain1_strategy.save_to }}')

# For complex JSON (use typ='series' for dict-like structure):
collected_data = pd.read_json('{{ requirement_analysis.chain1_strategy.save_to }}', typ='series').to_dict()
```

{% elif requirement_analysis.requirement_type == "synthesis" %}
### Dependency Result Files (From Previous Requirements)

This is a **synthesis** requirement. You need to load and integrate results from:

{% if requirement_analysis.chain2_strategy.input_result_files %}
{% for file_path in requirement_analysis.chain2_strategy.input_result_files %}
- **{{ file_path }}**
{% endfor %}
{% elif requirement_analysis.chain2_strategy.depends_on %}
{% for dep_id in requirement_analysis.chain2_strategy.depends_on %}
- **{{ experiment_dir }}/data/req_{{ dep_id }}/analysis/results.json**
{% endfor %}
{% endif %}

**How to load dependency results**:
```python
import pandas as pd

{% if requirement_analysis.chain2_strategy.depends_on %}
{% for dep_id in requirement_analysis.chain2_strategy.depends_on %}
# Load requirement {{ dep_id }} results
req_{{ dep_id }}_df = pd.read_json('{{ experiment_dir }}/data/req_{{ dep_id }}/analysis/results.json')
{% endfor %}
{% endif %}

# Now integrate: req_1_df, req_2_df, etc.
# Use pd.merge() or pd.concat() to combine DataFrames
```

{% endif %}

================================================================================
DATA OUTPUT
================================================================================

**Save Location**: {{ requirement_analysis.chain2_strategy.save_to | default(experiment_dir ~ '/data/req_' ~ requirement_analysis.requirement_id ~ '/analysis/results.json') }}

### ‚ö†Ô∏è SECURITY RESTRICTIONS (PANDAS CODE)

**FORBIDDEN OPERATIONS** - These will cause errors in `run_pandas_code_tool`:

```python
# ‚ùå FORBIDDEN - Will be blocked:
import os           # Operating system access
import sys          # System access
import subprocess   # Shell command execution
os.makedirs(...)    # Directory creation
os.path.exists(...) # Path checking
open(...)           # File operations (use pd.read_* instead)
exec(...)           # Dynamic code execution
eval(...)           # Expression evaluation
```

**USE INSTEAD** - Only pandas/numpy operations are allowed:

```python
# ‚úÖ CORRECT - Allowed operations:
import pandas as pd
import numpy as np
from scipy import stats

# CSV files
df = pd.read_csv('/path/to/file.csv')       # Read CSV
df.to_csv('/path/to/output.csv', index=False)  # Write CSV

# JSON files (use pd.read_json / to_json)
df = pd.read_json('/path/to/input.json')    # Read JSON as DataFrame
df.to_json('/path/to/output.json', orient='records', indent=2)  # Write JSON

# For complex JSON (not tabular):
# Store result in DataFrame first, then export
result_df = pd.DataFrame([{
    'analysis_type': 'correlation',
    'gene_pairs': gene_pairs_list,
    'metadata': metadata_dict
}])
result_df.to_json('/path/to/results.json', orient='records', indent=2)
```

**Note**: Output directories are pre-created. Do NOT try to create directories.

### CRITICAL: NO TRUNCATION

**You MUST save complete analysis results. Do NOT truncate!**

```python
# WRONG - Truncated results
results_df = pd.DataFrame([
    {"top_pairs": "gene1-gene2, gene3-gene4, ..."},  # ‚ùå Truncated!
])

# CORRECT - Full results using DataFrame
import pandas as pd

# Create DataFrame with ALL gene pairs
gene_pairs = [
    {"gene1": "Cd4", "gene2": "Lck", "correlation": 0.95, "p_value": 0.001},
    {"gene1": "Cd8a", "gene2": "Zap70", "correlation": 0.92, "p_value": 0.002},
    # ... ALL 500 gene pairs - DO NOT TRUNCATE!
]

results_df = pd.DataFrame(gene_pairs)

# Add metadata as columns if needed
results_df['analysis_type'] = 'correlation'
results_df['method'] = 'pearson'
results_df['threshold'] = 0.8

# Save full results using pandas
results_df.to_json('{{ requirement_analysis.chain2_strategy.save_to | default("results.json") }}', orient='records', indent=2)

print(f"Saved {len(results_df)} gene pairs to results.json")
```

================================================================================
PLANNED ANALYSIS TOOLS
================================================================================

{% if requirement_analysis.chain2_strategy.tools %}
Execute these tools for analysis:

{% for tool in requirement_analysis.chain2_strategy.tools %}
### Tool {{ loop.index }}: {{ tool.server }}.{{ tool.tool }}

**Purpose**: {{ tool.purpose }}
{% if tool.arguments %}
**Arguments**:
```json
{{ tool.arguments | tojson(indent=2) }}
```
{% endif %}
{% if tool.code_hint %}
**Code Hint**:
```python
{{ tool.code_hint }}
```
{% endif %}
**Required**: {{ tool.required | default(true) }}

{% endfor %}
{% endif %}

================================================================================
AVAILABLE ANALYSIS TOOLS
================================================================================

{{ tool_names }}

### Tool Reference by File Type

| File Type | Server | Key Tools |
|-----------|--------|-----------|
| **CSV** | pandas_analysis | `read_metadata_tool`, `run_pandas_code_tool`, `interpret_column_data` |
| **BAM** | nanopore | `get_alignment_stats`, `analyze_polya_lengths`, `compare_polya_distributions` |
| **POD5** | nanopore | `read_pod5_info`, `detect_modified_bases`, `analyze_polya_signal` |
| **JSON** | pandas_analysis | `run_pandas_code_tool` (use pd.read_json) |
| **PDB** | rosetta, vina | `calculate_energy`, `dock_proteins`, `dock_ligand` |
| **FASTA** | blast, msa | `find_similar_proteins`, `align_sequences` |

### Tool Reference by Analysis Type

| Analysis Type | Server | Key Tools |
|---------------|--------|-----------|
| **correlation** | pandas_analysis | `run_pandas_code_tool` with df.corr() |
| **comparison** | pandas_analysis, nanopore | Statistical tests, compare_polya_distributions |
| **network** | networkx | `build_network`, `find_hub_proteins`, `calculate_centrality` |
| **enrichment** | gprofiler | `enrichment_analysis` |
| **structure** | esmfold, foldseek | `predict_structure`, `search_structure` |
| **integration** | pandas_analysis | `run_pandas_code_tool` with pd.read_json + merge |

### ‚ö†Ô∏è CRITICAL: run_pandas_code_tool Usage

**The `code` parameter is REQUIRED.** You MUST provide Python code as a string.

**Correct tool call format:**
```json
{
  "name": "run_pandas_code_tool",
  "arguments": {
    "code": "import pandas as pd\ndf = pd.read_csv('/path/to/file.csv')\nprint(df.head())\ndf.to_json('/path/to/output.json', orient='records', indent=2)"
  }
}
```

**WRONG (missing code parameter):**
```json
{
  "name": "run_pandas_code_tool",
  "arguments": {}
}
```

================================================================================
ReAct REASONING FRAMEWORK
================================================================================

Follow this iterative process:

### Step 1: ASSESS
- Review the analysis plan above
- Check available input files or collected data
- Understand what outputs are expected

### Step 2: REASON
Based on requirement type:
- **input_only**: Analyze input files directly using appropriate tools
- **collection_required**: Load collected JSON, then analyze
- **synthesis**: Load dependency results, then integrate

### Step 3: ACT
- Call the planned tools in sequence
- Use absolute file paths
- Save intermediate results if needed

### Step 4: OBSERVE
- Check tool output
- Verify data quality and completeness
- Determine if more analysis is needed

### Step 5: ITERATE or CONCLUDE
- If more steps remain ‚Üí continue
- If complete ‚Üí save full results and summarize

================================================================================
ANALYSIS EXAMPLES
================================================================================

{% if requirement_analysis.requirement_type == "input_only" %}
### Example: Expression Correlation (input_only + CSV)

```
THOUGHT: I need to analyze expression data from CSV and compute gene correlations.

ACTION: pandas_analysis.read_metadata_tool(file_path="/path/to/expression.csv")
OBSERVATION: DataFrame with 300 genes x 10 samples, columns: gene_name, sample1, sample2, ...

THOUGHT: Now compute Pearson correlation between genes.
ACTION: pandas_analysis.run_pandas_code_tool(code="""
import pandas as pd
df = pd.read_csv('/path/to/expression.csv', index_col='gene_name')
corr_matrix = df.T.corr()
# Get top pairs
pairs = []
for i in range(len(corr_matrix)):
    for j in range(i+1, len(corr_matrix)):
        pairs.append({
            'gene1': corr_matrix.index[i],
            'gene2': corr_matrix.columns[j],
            'correlation': corr_matrix.iloc[i,j]
        })
# Sort and save
pairs_df = pd.DataFrame(pairs).sort_values('correlation', ascending=False)
pairs_df.to_json('/path/to/results.json', orient='records')
print(f'Saved {len(pairs_df)} gene pairs')
""")
OBSERVATION: Saved 44850 gene pairs

CONCLUSION: Computed correlation matrix for 300 genes, saved 44850 gene pairs to results.json
```

{% elif requirement_analysis.requirement_type == "synthesis" %}
### Example: Integrating Multiple Requirement Results (synthesis)

```
THOUGHT: This is a synthesis requirement. I need to load results from requirements 1, 2, 3 and integrate them.

ACTION: pandas_analysis.run_pandas_code_tool(code="""
import pandas as pd

# Load all dependency results using pd.read_json
df1 = pd.read_json('/path/to/req_1/analysis/results.json')  # Function similarity scores
df2 = pd.read_json('/path/to/req_2/analysis/results.json')  # Phylogenetic similarity
df3 = pd.read_json('/path/to/req_3/analysis/results.json')  # Expression correlation

# Merge on gene pairs
merged = df1.merge(df2, on=['gene1', 'gene2'], suffixes=('_func', '_phylo'))
merged = merged.merge(df3, on=['gene1', 'gene2'])

# Compute integrated score (weighted average)
merged['integrated_score'] = (
    0.3 * merged['function_score'] +
    0.3 * merged['phylo_score'] +
    0.4 * merged['correlation']
)

# Rank and get top 100 pairs
top_pairs = merged.nlargest(100, 'integrated_score')

# Add metadata columns
top_pairs['weight_function'] = 0.3
top_pairs['weight_phylo'] = 0.3
top_pairs['weight_expression'] = 0.4
top_pairs['total_pairs_analyzed'] = len(merged)

# Save using pandas (NO open() or json.dump!)
top_pairs.to_json('/path/to/req_4/analysis/results.json', orient='records', indent=2)

print(f'Integrated {len(merged)} pairs, saved top 100')
""")
OBSERVATION: Integrated 44850 pairs, saved top 100

CONCLUSION: Successfully integrated results from 3 requirements, identified top 100 gene pairs by combined similarity.
```

{% elif requirement_analysis.requirement_type == "collection_required" %}
### Example: Analyzing Collected PPI Data (collection_required)

```
THOUGHT: Chain 1 collected PPI data from STRING. Now I need to build and analyze the network.

ACTION: pandas_analysis.run_pandas_code_tool(code="""
import pandas as pd

# Load collected data using pd.read_json
collected_df = pd.read_json('/path/to/collection/sources.json')

# If data is nested, access it via column
# For complex JSON structures, use typ='series'
collected_series = pd.read_json('/path/to/collection/sources.json', typ='series')
interactions = collected_series['stringdb_results']['interactions']
print(f'Loaded {len(interactions)} interactions')
""")
OBSERVATION: Loaded 150 interactions

ACTION: networkx.build_network(edges=interactions)
OBSERVATION: Network built with 45 nodes, 150 edges

ACTION: networkx.find_hub_proteins(top_n=10)
OBSERVATION: Hub proteins: STAT3 (degree 15), JAK1 (degree 12), ...

CONCLUSION: Identified 10 hub proteins in the T cell signaling network.
```

{% endif %}

================================================================================
BEGIN ANALYSIS
================================================================================

{% if requirement_analysis.chain2_strategy.multi_step %}
Follow the multi-step plan above:
{% for step_info in requirement_analysis.chain2_strategy.multi_step %}
{{ loop.index }}. {{ step_info.description }}
{% endfor %}
{% endif %}

Execute the planned tools and save COMPLETE results (no truncation!) to:
**{{ requirement_analysis.chain2_strategy.save_to | default(experiment_dir ~ '/data/req_' ~ requirement_analysis.requirement_id ~ '/analysis/results.json') }}**
